# ðŸ¤– AIå·¥ç¨‹å¸ˆæŠ€èƒ½é€ŸæŸ¥å¡

> æœºå™¨å­¦ä¹ ã€NLPã€OCRæ ¸å¿ƒçŸ¥è¯†ç‚¹å¿«é€Ÿå‚è€ƒ

---

## ðŸ§  æœºå™¨å­¦ä¹ åŸºç¡€

### ç‰¹å¾å·¥ç¨‹

#### 1. ç±»åˆ«ç¼–ç 

```python
from sklearn.preprocessing import LabelEncoder, OneHotEncoder

# Labelç¼–ç ï¼ˆè½¬ä¸ºæ•´æ•°ï¼‰
le = LabelEncoder()
encoded = le.fit_transform(['high', 'low', 'medium'])  # [0, 1, 2]

# One-Hotç¼–ç 
ohe = OneHotEncoder(sparse_output=False)
values_2d = np.array(['A', 'B', 'A']).reshape(-1, 1)
encoded = ohe.fit_transform(values_2d)  # [[1,0], [0,1], [1,0]]

# pandas get_dummiesï¼ˆæ›´ç®€å•ï¼‰
import pandas as pd
df = pd.DataFrame({'category': ['A', 'B', 'A']})
encoded = pd.get_dummies(df['category'])
```

#### 2. æ•°å€¼å½’ä¸€åŒ–

```python
from sklearn.preprocessing import StandardScaler, MinMaxScaler, RobustScaler

# æ ‡å‡†åŒ–ï¼š(x - mean) / std
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)  # æ³¨æ„ï¼šç”¨transformè€Œéžfit_transform

# Min-Maxå½’ä¸€åŒ–ï¼šç¼©æ”¾åˆ°[0, 1]
scaler = MinMaxScaler(feature_range=(0, 1))
X_scaled = scaler.fit_transform(X)

# Robustå½’ä¸€åŒ–ï¼šå¯¹å¼‚å¸¸å€¼é²æ£’
scaler = RobustScaler()
X_scaled = scaler.fit_transform(X)
```

#### 3. æ•°å€¼åˆ†ç®±

```python
import pandas as pd
from sklearn.preprocessing import KBinsDiscretizer

# pandas.cutï¼ˆç­‰å®½åˆ†ç®±ï¼‰
bins = pd.cut([1, 5, 10, 15, 20], bins=3, labels=False)  # [0, 0, 1, 2, 2]

# pandas.qcutï¼ˆç­‰é¢‘åˆ†ç®±ï¼‰
bins = pd.qcut([1, 2, 3, 4, 5, 6, 7, 8, 9, 10], q=4, labels=False)

# sklearn KBinsDiscretizer
kbd = KBinsDiscretizer(n_bins=5, encode='ordinal', strategy='quantile')
X_binned = kbd.fit_transform(X)
```

---

### æ¨¡åž‹è®­ç»ƒ

#### 1. åˆ†ç±»å™¨

```python
from sklearn.linear_model import LogisticRegression
from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier
from sklearn.svm import SVC

# é€»è¾‘å›žå½’
model = LogisticRegression(random_state=42, max_iter=1000)
model.fit(X_train, y_train)

# å†³ç­–æ ‘
model = DecisionTreeClassifier(max_depth=5, random_state=42)
model.fit(X_train, y_train)

# éšæœºæ£®æž—
model = RandomForestClassifier(n_estimators=100, random_state=42)
model.fit(X_train, y_train)

# æ¢¯åº¦æå‡æ ‘ï¼ˆGBDTï¼‰
model = GradientBoostingClassifier(n_estimators=100, random_state=42)
model.fit(X_train, y_train)

# æ”¯æŒå‘é‡æœº
model = SVC(kernel='rbf', probability=True, random_state=42)
model.fit(X_train, y_train)
```

#### 2. å›žå½’å™¨

```python
from sklearn.linear_model import LinearRegression, Ridge, Lasso
from sklearn.ensemble import RandomForestRegressor

# çº¿æ€§å›žå½’
model = LinearRegression()
model.fit(X_train, y_train)

# Ridgeå›žå½’ï¼ˆL2æ­£åˆ™åŒ–ï¼‰
model = Ridge(alpha=1.0)
model.fit(X_train, y_train)

# Lassoå›žå½’ï¼ˆL1æ­£åˆ™åŒ–ï¼‰
model = Lasso(alpha=1.0)
model.fit(X_train, y_train)

# éšæœºæ£®æž—å›žå½’
model = RandomForestRegressor(n_estimators=100, random_state=42)
model.fit(X_train, y_train)
```

---

### æ¨¡åž‹è¯„ä¼°

#### 1. åˆ†ç±»æŒ‡æ ‡

```python
from sklearn.metrics import (
    accuracy_score, precision_score, recall_score, f1_score,
    confusion_matrix, classification_report,
    roc_auc_score, roc_curve
)

# åŸºæœ¬æŒ‡æ ‡
accuracy = accuracy_score(y_true, y_pred)
precision = precision_score(y_true, y_pred, average='binary')  # æˆ– 'weighted'
recall = recall_score(y_true, y_pred, average='binary')
f1 = f1_score(y_true, y_pred, average='binary')

# æ··æ·†çŸ©é˜µ
cm = confusion_matrix(y_true, y_pred)
# [[TN, FP],
#  [FN, TP]]

# åˆ†ç±»æŠ¥å‘Š
report = classification_report(y_true, y_pred)

# AUCï¼ˆéœ€è¦é¢„æµ‹æ¦‚çŽ‡ï¼‰
y_prob = model.predict_proba(X_test)[:, 1]  # æ­£ç±»æ¦‚çŽ‡
auc = roc_auc_score(y_true, y_prob)

# ROCæ›²çº¿
fpr, tpr, thresholds = roc_curve(y_true, y_prob)
```

#### 2. å›žå½’æŒ‡æ ‡

```python
from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score

# MSEï¼ˆå‡æ–¹è¯¯å·®ï¼‰
mse = mean_squared_error(y_true, y_pred)

# RMSEï¼ˆå‡æ–¹æ ¹è¯¯å·®ï¼‰
rmse = np.sqrt(mse)

# MAEï¼ˆå¹³å‡ç»å¯¹è¯¯å·®ï¼‰
mae = mean_absolute_error(y_true, y_pred)

# RÂ²ï¼ˆå†³å®šç³»æ•°ï¼‰
r2 = r2_score(y_true, y_pred)
```

#### 3. äº¤å‰éªŒè¯

```python
from sklearn.model_selection import cross_val_score, cross_validate

# ç®€å•äº¤å‰éªŒè¯
scores = cross_val_score(model, X, y, cv=5, scoring='accuracy')
print(f"Mean: {scores.mean():.4f}, Std: {scores.std():.4f}")

# å¤šæŒ‡æ ‡äº¤å‰éªŒè¯
scoring = ['accuracy', 'precision', 'recall', 'f1']
scores = cross_validate(model, X, y, cv=5, scoring=scoring)
```

---

### æ•°æ®åˆ†å‰²

```python
from sklearn.model_selection import train_test_split

# ç®€å•åˆ†å‰²
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=42
)

# åˆ†å±‚åˆ†å‰²ï¼ˆä¿æŒç±»åˆ«æ¯”ä¾‹ï¼‰
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, stratify=y, random_state=42
)
```

---

## ðŸ“ NLPåŸºç¡€

### ä¸­æ–‡åˆ†è¯

```python
import jieba

# åŸºæœ¬åˆ†è¯
text = "å¢žå€¼ç¨Žä¸“ç”¨å‘ç¥¨"
tokens = jieba.lcut(text)  # ['å¢žå€¼ç¨Ž', 'ä¸“ç”¨', 'å‘ç¥¨']

# æ·»åŠ è‡ªå®šä¹‰è¯å…¸
jieba.add_word("å¢žå€¼ç¨Žä¸“ç”¨å‘ç¥¨")
tokens = jieba.lcut(text)  # ['å¢žå€¼ç¨Žä¸“ç”¨å‘ç¥¨']

# å…³é”®è¯æå–
import jieba.analyse
keywords = jieba.analyse.extract_tags(text, topK=5)
```

### åœç”¨è¯è¿‡æ»¤

```python
# åŠ è½½åœç”¨è¯è¡¨
with open('stopwords.txt', 'r', encoding='utf-8') as f:
    stopwords = set(f.read().splitlines())

# è¿‡æ»¤
tokens = [w for w in tokens if w not in stopwords]
```

### TF-IDF

```python
from sklearn.feature_extraction.text import TfidfVectorizer

# åˆ›å»ºå‘é‡åŒ–å™¨
vectorizer = TfidfVectorizer(max_features=1000)

# è®­ç»ƒå¹¶è½¬æ¢
texts = ["æ–‡æœ¬1", "æ–‡æœ¬2", "æ–‡æœ¬3"]
X = vectorizer.fit_transform(texts)

# èŽ·å–ç‰¹å¾å
feature_names = vectorizer.get_feature_names_out()
```

### æ–‡æœ¬ç›¸ä¼¼åº¦

```python
from sklearn.metrics.pairwise import cosine_similarity

# ä½™å¼¦ç›¸ä¼¼åº¦
sim = cosine_similarity(vec1, vec2)

# ç¼–è¾‘è·ç¦»
from difflib import SequenceMatcher
ratio = SequenceMatcher(None, text1, text2).ratio()
```

---

## ðŸ–¼ï¸ OCRåŸºç¡€

### PaddleOCR

```python
from paddleocr import PaddleOCR

# åˆå§‹åŒ–
ocr = PaddleOCR(use_angle_cls=True, lang='ch')

# è¯†åˆ«
result = ocr.ocr(img_path, cls=True)

# è§£æžç»“æžœ
for line in result[0]:
    box = line[0]  # åæ ‡
    text = line[1][0]  # æ–‡æœ¬
    confidence = line[1][1]  # ç½®ä¿¡åº¦
    print(f"{text} ({confidence:.2f})")
```

### Tesseract

```python
import pytesseract
from PIL import Image

# è¯†åˆ«
img = Image.open(img_path)
text = pytesseract.image_to_string(img, lang='chi_sim')

# èŽ·å–è¯¦ç»†ä¿¡æ¯
data = pytesseract.image_to_data(img, lang='chi_sim', output_type=pytesseract.Output.DICT)
```

### å›¾åƒé¢„å¤„ç†

```python
import cv2

# è¯»å–å›¾åƒ
img = cv2.imread(img_path)

# ç°åº¦åŒ–
gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)

# äºŒå€¼åŒ–
_, binary = cv2.threshold(gray, 127, 255, cv2.THRESH_BINARY)

# åŽ»å™ª
denoised = cv2.fastNlMeansDenoising(gray)

# å€¾æ–œæ ¡æ­£
coords = np.column_stack(np.where(binary > 0))
angle = cv2.minAreaRect(coords)[-1]
if angle < -45:
    angle = -(90 + angle)
else:
    angle = -angle
(h, w) = img.shape[:2]
center = (w // 2, h // 2)
M = cv2.getRotationMatrix2D(center, angle, 1.0)
rotated = cv2.warpAffine(img, M, (w, h), flags=cv2.INTER_CUBIC, borderMode=cv2.BORDER_REPLICATE)
```

---

## ðŸŽ¯ å¸¸è§é™·é˜±

### 1. æ•°æ®æ³„éœ²

```python
# âŒ é”™è¯¯ï¼šåœ¨åˆ†å‰²å‰å½’ä¸€åŒ–
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)
X_train, X_test = train_test_split(X_scaled, ...)

# âœ… æ­£ç¡®ï¼šå…ˆåˆ†å‰²ï¼Œå†å½’ä¸€åŒ–
X_train, X_test = train_test_split(X, ...)
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)  # ç”¨transformè€Œéžfit_transform
```

### 2. ç±»åˆ«ä¸å¹³è¡¡

```python
# æ–¹æ³•1ï¼šè°ƒæ•´ç±»åˆ«æƒé‡
model = LogisticRegression(class_weight='balanced')

# æ–¹æ³•2ï¼šè¿‡é‡‡æ ·ï¼ˆSMOTEï¼‰
from imblearn.over_sampling import SMOTE
smote = SMOTE(random_state=42)
X_resampled, y_resampled = smote.fit_resample(X_train, y_train)

# æ–¹æ³•3ï¼šæ¬ é‡‡æ ·
from imblearn.under_sampling import RandomUnderSampler
rus = RandomUnderSampler(random_state=42)
X_resampled, y_resampled = rus.fit_resample(X_train, y_train)
```

### 3. è¿‡æ‹Ÿåˆ

```python
# æ–¹æ³•1ï¼šæ­£åˆ™åŒ–
model = Ridge(alpha=1.0)  # L2
model = Lasso(alpha=1.0)  # L1

# æ–¹æ³•2ï¼šå‡å°‘æ¨¡åž‹å¤æ‚åº¦
model = DecisionTreeClassifier(max_depth=5)  # é™åˆ¶æ·±åº¦
model = RandomForestClassifier(max_features='sqrt')  # é™åˆ¶ç‰¹å¾æ•°

# æ–¹æ³•3ï¼šå¢žåŠ æ•°æ®
# æ•°æ®å¢žå¼ºã€æ”¶é›†æ›´å¤šæ•°æ®

# æ–¹æ³•4ï¼šDropoutï¼ˆæ·±åº¦å­¦ä¹ ï¼‰
# åœ¨ç¥žç»ç½‘ç»œä¸­æ·»åŠ Dropoutå±‚
```

---

## ðŸ“Š è¯„ä¼°æŒ‡æ ‡é€‰æ‹©

| åœºæ™¯ | æŽ¨èæŒ‡æ ‡ | åŽŸå›  |
|------|---------|------|
| ç±»åˆ«å¹³è¡¡ | Accuracy | ç®€å•ç›´è§‚ |
| ç±»åˆ«ä¸å¹³è¡¡ | F1, AUC | ç»¼åˆè€ƒè™‘ç²¾ç¡®çŽ‡å’Œå¬å›žçŽ‡ |
| å…³æ³¨è¯¯æŠ¥ | Precision | å‡å°‘å‡é˜³æ€§ |
| å…³æ³¨æ¼æŠ¥ | Recall | å‡å°‘å‡é˜´æ€§ |
| æŽ’åºè´¨é‡ | AUC | è¯„ä¼°æ¨¡åž‹åŒºåˆ†èƒ½åŠ› |
| å›žå½’ä»»åŠ¡ | RMSE, MAE | è¯¯å·®çš„ç»å¯¹å€¼ |

---

**å¿«é€Ÿè®°å¿†å£è¯€**ï¼š

- ç‰¹å¾å·¥ç¨‹ï¼š**ç¼–ç ç±»åˆ«ï¼Œå½’ä¸€æ•°å€¼ï¼Œåˆ†ç®±ç¦»æ•£**
- æ¨¡åž‹è®­ç»ƒï¼š**å…ˆåˆ†å‰²ï¼Œå†å½’ä¸€ï¼Œæœ€åŽè®­ç»ƒ**
- æ¨¡åž‹è¯„ä¼°ï¼š**å‡†ç¡®çŽ‡çœ‹æ•´ä½“ï¼ŒF1çœ‹å¹³è¡¡ï¼ŒAUCçœ‹æŽ’åº**
- NLPï¼š**åˆ†è¯åŽ»åœï¼Œå‘é‡åŒ–ï¼Œç®—ç›¸ä¼¼**
- OCRï¼š**é¢„å¤„ç†å›¾åƒï¼Œè¯†åˆ«æ–‡æœ¬ï¼ŒåŽå¤„ç†æ ¡éªŒ**


# ğŸ¤– AIå·¥ç¨‹å¸ˆæŠ€èƒ½ç¼ºå£åˆ†æä¸æ‰©å……æ–¹æ¡ˆ

## ğŸ“Š å½“å‰çŠ¶æ€è¯„ä¼°

### âœ… å·²è¦†ç›–çš„AIç›¸å…³æŠ€èƒ½

1. **æ•°æ®å¤„ç†åŸºç¡€** â­â­â­â­â­
   - pandas æ•°æ®æ¸…æ´—ã€èšåˆã€é€è§†ï¼ˆSet B, Gï¼‰
   - NumPy å‘é‡åŒ–è®¡ç®—ï¼ˆSet Dï¼‰
   - æ•°æ®ç±»å‹è½¬æ¢ã€ç¼ºå¤±å€¼å¤„ç†
   - æ—¶é—´åºåˆ—å¤„ç†ï¼ˆç¯æ¯”ã€åŒæ¯”ï¼‰

2. **æ–‡æœ¬å¤„ç†** â­â­â­â­
   - æ­£åˆ™è¡¨è¾¾å¼ï¼ˆSet A, E, F, Vï¼‰
   - OCRæ–‡æœ¬æ¸…æ´—ï¼ˆSet V - ä»…æ¨¡æ‹Ÿï¼‰
   - å­—ç¬¦ä¸²è§„èŒƒåŒ–

3. **å·¥ç¨‹èƒ½åŠ›** â­â­â­â­
   - å¹¶å‘ç¼–ç¨‹ï¼ˆasyncio/threadingï¼‰
   - APIè®¾è®¡ä¸æœåŠ¡åŒ–
   - æ—¥å¿—ä¸å¯è§‚æµ‹æ€§
   - å¼‚å¸¸å¤„ç†

### âŒ ä¸¥é‡ç¼ºå¤±çš„AIæ ¸å¿ƒæŠ€èƒ½

#### 1. **æœºå™¨å­¦ä¹ åŸºç¡€** ğŸ”´ å®Œå…¨ç¼ºå¤±

**ç¼ºå¤±å†…å®¹**ï¼š
- âŒ ç‰¹å¾å·¥ç¨‹ï¼ˆç¼–ç ã€å½’ä¸€åŒ–ã€åˆ†ç®±ã€ç‰¹å¾é€‰æ‹©ï¼‰
- âŒ æ¨¡å‹è®­ç»ƒä¸è¯„ä¼°ï¼ˆsklearnåŸºç¡€ï¼‰
- âŒ äº¤å‰éªŒè¯ä¸è¶…å‚æ•°è°ƒä¼˜
- âŒ æ¨¡å‹è¯„ä¼°æŒ‡æ ‡ï¼ˆå‡†ç¡®ç‡ã€ç²¾ç¡®ç‡ã€å¬å›ç‡ã€F1ã€AUCï¼‰
- âŒ è¿‡æ‹Ÿåˆä¸æ­£åˆ™åŒ–
- âŒ æ¨¡å‹æŒä¹…åŒ–ï¼ˆpickle/joblibï¼‰

**å½±å“**ï¼š
- æ— æ³•è¯„ä¼°å€™é€‰äººçš„MLåŸºç¡€
- ç¼ºå°‘å®é™…å»ºæ¨¡èƒ½åŠ›è€ƒå¯Ÿ
- ä¸ç¬¦åˆ"AIå·¥ç¨‹å¸ˆ"å²—ä½å®šä½

#### 2. **æ·±åº¦å­¦ä¹ åŸºç¡€** ğŸ”´ å®Œå…¨ç¼ºå¤±

**ç¼ºå¤±å†…å®¹**ï¼š
- âŒ å¼ é‡æ“ä½œï¼ˆPyTorch/TensorFlowåŸºç¡€ï¼‰
- âŒ ç¥ç»ç½‘ç»œåŸºæœ¬æ¦‚å¿µ
- âŒ æŸå¤±å‡½æ•°ä¸ä¼˜åŒ–å™¨
- âŒ æ¨¡å‹è®­ç»ƒå¾ªç¯
- âŒ GPUåŠ é€ŸåŸºç¡€

**å½±å“**ï¼š
- æ— æ³•è¯„ä¼°æ·±åº¦å­¦ä¹ èƒ½åŠ›
- ç°ä»£AIå²—ä½å¿…å¤‡æŠ€èƒ½ç¼ºå¤±

#### 3. **NLPä¸“é¡¹** ğŸŸ¡ ä¸¥é‡ä¸è¶³

**ç°çŠ¶**ï¼š
- âœ… ä»…æœ‰åŸºç¡€æ–‡æœ¬å¤„ç†ï¼ˆæ­£åˆ™ã€æ¸…æ´—ï¼‰
- âŒ ç¼ºå°‘åˆ†è¯ã€è¯å‘é‡
- âŒ ç¼ºå°‘æ–‡æœ¬åˆ†ç±»ã€æƒ…æ„Ÿåˆ†æ
- âŒ ç¼ºå°‘å‘½åå®ä½“è¯†åˆ«ï¼ˆNERï¼‰
- âŒ ç¼ºå°‘æ–‡æœ¬ç›¸ä¼¼åº¦è®¡ç®—

**ç¨åŠ¡åœºæ™¯éœ€æ±‚**ï¼š
- å‘ç¥¨æ–‡æœ¬åˆ†ç±»
- ç¨åŠ¡æ”¿ç­–æ–‡æœ¬ç†è§£
- åˆåŒå…³é”®ä¿¡æ¯æå–
- ç¨åŠ¡é—®ç­”ç³»ç»Ÿ

#### 4. **è®¡ç®—æœºè§†è§‰ï¼ˆOCRï¼‰** ğŸŸ¡ ä¸¥é‡ä¸è¶³

**ç°çŠ¶**ï¼š
- âœ… Set V æœ‰OCRæ–‡æœ¬æ¸…æ´—ï¼ˆä½†ä»…æ¨¡æ‹Ÿï¼‰
- âŒ ç¼ºå°‘å®é™…OCRè°ƒç”¨ï¼ˆTesseract/PaddleOCRï¼‰
- âŒ ç¼ºå°‘å›¾åƒé¢„å¤„ç†
- âŒ ç¼ºå°‘OCRç»“æœåå¤„ç†
- âŒ ç¼ºå°‘è¡¨æ ¼è¯†åˆ«

**ç¨åŠ¡åœºæ™¯éœ€æ±‚**ï¼š
- å‘ç¥¨OCRè¯†åˆ«
- è¡¨æ ¼ç»“æ„åŒ–æå–
- å°ç« æ£€æµ‹
- å›¾åƒè´¨é‡è¯„ä¼°

#### 5. **æ¨¡å‹éƒ¨ç½²ä¸æœåŠ¡åŒ–** ğŸŸ¡ éƒ¨åˆ†ç¼ºå¤±

**ç°çŠ¶**ï¼š
- âœ… æœ‰APIè®¾è®¡åŸºç¡€ï¼ˆSet S, U, ABï¼‰
- âŒ ç¼ºå°‘æ¨¡å‹æ¨ç†æœåŠ¡
- âŒ ç¼ºå°‘æ‰¹é‡é¢„æµ‹
- âŒ ç¼ºå°‘æ¨¡å‹ç‰ˆæœ¬ç®¡ç†
- âŒ ç¼ºå°‘A/Bæµ‹è¯•

#### 6. **æ•°æ®æ ‡æ³¨ä¸è´¨é‡** ğŸ”´ å®Œå…¨ç¼ºå¤±

**ç¼ºå¤±å†…å®¹**ï¼š
- âŒ æ•°æ®æ ‡æ³¨æµç¨‹
- âŒ æ ‡æ³¨è´¨é‡è¯„ä¼°
- âŒ ä¸»åŠ¨å­¦ä¹ 
- âŒ æ•°æ®å¢å¼º

---

## ğŸ¯ æ‰©å……æ–¹æ¡ˆï¼ˆæŒ‰ä¼˜å…ˆçº§ï¼‰

### ğŸ”¥ P0 - æ ¸å¿ƒAIæŠ€èƒ½ï¼ˆå¿…é¡»æ·»åŠ ï¼‰

#### 1. æœºå™¨å­¦ä¹ åŸºç¡€å¥—é¢˜ï¼ˆSet ML1ï¼‰

**é¢˜ç›®è®¾è®¡**ï¼š
```python
# 1. ç‰¹å¾å·¥ç¨‹
def encode_categorical(df, col: str, method: str = "onehot"):
    """ç±»åˆ«ç¼–ç ï¼šonehot/label/target"""
    
def normalize_features(df, cols: List[str], method: str = "standard"):
    """ç‰¹å¾å½’ä¸€åŒ–ï¼šstandard/minmax/robust"""
    
def create_bins(series, bins: int = 5, labels=None):
    """æ•°å€¼åˆ†ç®±"""

# 2. æ¨¡å‹è®­ç»ƒä¸è¯„ä¼°
def train_classifier(X_train, y_train, model_type: str = "logistic"):
    """è®­ç»ƒåˆ†ç±»å™¨ï¼šlogistic/tree/rf"""
    
def evaluate_model(y_true, y_pred, y_prob=None) -> dict:
    """è®¡ç®—è¯„ä¼°æŒ‡æ ‡ï¼šaccuracy/precision/recall/f1/auc"""
    
def cross_validate_model(X, y, model, cv: int = 5) -> dict:
    """äº¤å‰éªŒè¯"""

# 3. æ¨¡å‹æŒä¹…åŒ–
def save_model(model, path: str):
    """ä¿å­˜æ¨¡å‹"""
    
def load_and_predict(model_path: str, X) -> np.ndarray:
    """åŠ è½½æ¨¡å‹å¹¶é¢„æµ‹"""
```

**ä¸šåŠ¡åœºæ™¯**ï¼š
- ç¨åŠ¡é£é™©åˆ†ç±»ï¼ˆé«˜/ä¸­/ä½é£é™©ï¼‰
- å‘ç¥¨çœŸä¼ªåˆ¤æ–­
- ä¼ä¸šä¿¡ç”¨è¯„åˆ†

#### 2. NLPåŸºç¡€å¥—é¢˜ï¼ˆSet NLP1ï¼‰

**é¢˜ç›®è®¾è®¡**ï¼š
```python
# 1. æ–‡æœ¬é¢„å¤„ç†
def tokenize_chinese(text: str) -> List[str]:
    """ä¸­æ–‡åˆ†è¯ï¼ˆjiebaï¼‰"""
    
def remove_stopwords(tokens: List[str]) -> List[str]:
    """å»é™¤åœç”¨è¯"""
    
def extract_keywords(text: str, topk: int = 5) -> List[str]:
    """å…³é”®è¯æå–ï¼ˆTF-IDF/TextRankï¼‰"""

# 2. æ–‡æœ¬ç‰¹å¾
def text_to_tfidf(texts: List[str]) -> np.ndarray:
    """TF-IDFå‘é‡åŒ–"""
    
def compute_text_similarity(text1: str, text2: str) -> float:
    """æ–‡æœ¬ç›¸ä¼¼åº¦ï¼ˆä½™å¼¦/ç¼–è¾‘è·ç¦»ï¼‰"""

# 3. æ–‡æœ¬åˆ†ç±»
def train_text_classifier(texts: List[str], labels: List[int]):
    """è®­ç»ƒæ–‡æœ¬åˆ†ç±»å™¨"""
    
def extract_entities(text: str) -> List[Dict]:
    """å‘½åå®ä½“è¯†åˆ«ï¼ˆé‡‘é¢/æ—¥æœŸ/å…¬å¸åï¼‰"""
```

**ä¸šåŠ¡åœºæ™¯**ï¼š
- å‘ç¥¨æè¿°åˆ†ç±»ï¼ˆå•†å“ç±»åˆ«ï¼‰
- ç¨åŠ¡æ”¿ç­–æ–‡æœ¬æ£€ç´¢
- åˆåŒå…³é”®ä¿¡æ¯æå–
- å‘ç¥¨æŠ¬å¤´æ ‡å‡†åŒ–

#### 3. OCRå®æˆ˜å¥—é¢˜ï¼ˆSet OCR1ï¼‰

**é¢˜ç›®è®¾è®¡**ï¼š
```python
# 1. å›¾åƒé¢„å¤„ç†
def preprocess_invoice_image(img_path: str) -> np.ndarray:
    """å‘ç¥¨å›¾åƒé¢„å¤„ç†ï¼šç°åº¦åŒ–/äºŒå€¼åŒ–/å»å™ª/å€¾æ–œæ ¡æ­£"""
    
def detect_text_regions(img: np.ndarray) -> List[Tuple]:
    """æ–‡æœ¬åŒºåŸŸæ£€æµ‹"""

# 2. OCRè¯†åˆ«
def ocr_invoice(img_path: str, engine: str = "paddleocr") -> List[Dict]:
    """å‘ç¥¨OCRè¯†åˆ«"""
    
def extract_invoice_fields(ocr_result: List[Dict]) -> Dict:
    """ä»OCRç»“æœæå–ç»“æ„åŒ–å­—æ®µ"""

# 3. åå¤„ç†ä¸æ ¡éªŒ
def correct_ocr_errors(text: str, field_type: str) -> str:
    """OCRé”™è¯¯çº æ­£ï¼ˆåŸºäºè§„åˆ™/å­—å…¸ï¼‰"""
    
def validate_invoice_data(data: Dict) -> Tuple[bool, List[str]]:
    """å‘ç¥¨æ•°æ®æ ¡éªŒ"""
```

**ä¸šåŠ¡åœºæ™¯**ï¼š
- å¢å€¼ç¨å‘ç¥¨è¯†åˆ«
- è¡¨æ ¼å‹å‘ç¥¨ç»“æ„åŒ–
- å°ç« æ£€æµ‹ä¸éªŒè¯
- å¤šå¼ å‘ç¥¨æ‰¹é‡å¤„ç†

---

### ğŸ”¶ P1 - è¿›é˜¶AIæŠ€èƒ½ï¼ˆå¼ºçƒˆå»ºè®®ï¼‰

#### 4. æ·±åº¦å­¦ä¹ åŸºç¡€å¥—é¢˜ï¼ˆSet DL1ï¼‰

**é¢˜ç›®è®¾è®¡**ï¼š
```python
# 1. å¼ é‡æ“ä½œ
def tensor_operations():
    """PyTorchåŸºç¡€ï¼šåˆ›å»º/ç´¢å¼•/å˜å½¢/è¿ç®—"""
    
def build_simple_nn(input_dim: int, hidden_dim: int, output_dim: int):
    """æ„å»ºç®€å•ç¥ç»ç½‘ç»œ"""

# 2. è®­ç»ƒå¾ªç¯
def train_epoch(model, dataloader, optimizer, criterion):
    """è®­ç»ƒä¸€ä¸ªepoch"""
    
def evaluate_epoch(model, dataloader, criterion):
    """è¯„ä¼°ä¸€ä¸ªepoch"""

# 3. å®é™…åº”ç”¨
def train_invoice_classifier(train_data, val_data, epochs: int = 10):
    """è®­ç»ƒå‘ç¥¨åˆ†ç±»æ¨¡å‹"""
```

**ä¸šåŠ¡åœºæ™¯**ï¼š
- å‘ç¥¨å›¾åƒåˆ†ç±»
- æ–‡æœ¬æƒ…æ„Ÿåˆ†æ
- åºåˆ—æ ‡æ³¨ï¼ˆNERï¼‰

#### 5. æ¨¡å‹éƒ¨ç½²å¥—é¢˜ï¼ˆSet DEPLOY1ï¼‰

**é¢˜ç›®è®¾è®¡**ï¼š
```python
# 1. æ¨¡å‹æ¨ç†æœåŠ¡
def create_prediction_api(model_path: str, port: int = 8000):
    """åˆ›å»ºæ¨¡å‹æ¨ç†APIï¼ˆFastAPI/Flaskï¼‰"""
    
def batch_predict(model, data_path: str, batch_size: int = 32):
    """æ‰¹é‡é¢„æµ‹"""

# 2. æ€§èƒ½ä¼˜åŒ–
def optimize_inference(model):
    """æ¨ç†ä¼˜åŒ–ï¼šé‡åŒ–/å‰ªæ/ONNXè½¬æ¢"""
    
def cache_predictions(cache_key: str, ttl: int = 3600):
    """é¢„æµ‹ç»“æœç¼“å­˜"""

# 3. ç›‘æ§ä¸ç‰ˆæœ¬ç®¡ç†
def log_prediction_metrics(y_true, y_pred, model_version: str):
    """è®°å½•é¢„æµ‹æŒ‡æ ‡"""
    
def ab_test_models(model_a, model_b, traffic_split: float = 0.5):
    """A/Bæµ‹è¯•"""
```

#### 6. æ•°æ®å·¥ç¨‹å¥—é¢˜ï¼ˆSet DATA_ENG1ï¼‰

**é¢˜ç›®è®¾è®¡**ï¼š
```python
# 1. æ•°æ®æ ‡æ³¨
def create_annotation_task(data: List[Dict], task_type: str):
    """åˆ›å»ºæ ‡æ³¨ä»»åŠ¡"""
    
def validate_annotations(annotations: List[Dict]) -> float:
    """æ ‡æ³¨è´¨é‡è¯„ä¼°ï¼ˆä¸€è‡´æ€§/è¦†ç›–ç‡ï¼‰"""

# 2. æ•°æ®å¢å¼º
def augment_text(text: str, methods: List[str]) -> List[str]:
    """æ–‡æœ¬æ•°æ®å¢å¼ºï¼šåŒä¹‰è¯æ›¿æ¢/å›è¯‘/éšæœºæ’å…¥"""
    
def augment_image(img: np.ndarray) -> List[np.ndarray]:
    """å›¾åƒæ•°æ®å¢å¼ºï¼šæ—‹è½¬/ç¿»è½¬/å™ªå£°/äº®åº¦"""

# 3. ä¸»åŠ¨å­¦ä¹ 
def select_samples_for_annotation(model, unlabeled_data, n: int = 100):
    """ä¸»åŠ¨å­¦ä¹ æ ·æœ¬é€‰æ‹©ï¼ˆä¸ç¡®å®šæ€§é‡‡æ ·ï¼‰"""
```

---

### ğŸ”· P2 - é«˜çº§AIæŠ€èƒ½ï¼ˆæ—¶é—´å……è£•æ—¶ï¼‰

#### 7. å¤§æ¨¡å‹åº”ç”¨å¥—é¢˜ï¼ˆSet LLM1ï¼‰

**é¢˜ç›®è®¾è®¡**ï¼š
```python
# 1. Promptå·¥ç¨‹
def create_tax_qa_prompt(question: str, context: str) -> str:
    """æ„å»ºç¨åŠ¡é—®ç­”Prompt"""
    
def extract_info_with_llm(text: str, schema: Dict) -> Dict:
    """ä½¿ç”¨LLMæå–ç»“æ„åŒ–ä¿¡æ¯"""

# 2. RAGï¼ˆæ£€ç´¢å¢å¼ºç”Ÿæˆï¼‰
def build_tax_knowledge_base(documents: List[str]):
    """æ„å»ºç¨åŠ¡çŸ¥è¯†åº“ï¼ˆå‘é‡æ•°æ®åº“ï¼‰"""
    
def retrieve_and_generate(query: str, top_k: int = 3) -> str:
    """æ£€ç´¢ç›¸å…³æ–‡æ¡£å¹¶ç”Ÿæˆç­”æ¡ˆ"""

# 3. Fine-tuning
def finetune_llm_for_tax(train_data: List[Dict], base_model: str):
    """å¾®è°ƒLLMç”¨äºç¨åŠ¡åœºæ™¯"""
```

#### 8. æ—¶é—´åºåˆ—é¢„æµ‹å¥—é¢˜ï¼ˆSet TS1ï¼‰

**é¢˜ç›®è®¾è®¡**ï¼š
```python
# 1. æ—¶é—´åºåˆ—ç‰¹å¾
def create_lag_features(df, col: str, lags: List[int]):
    """åˆ›å»ºæ»åç‰¹å¾"""
    
def create_rolling_features(df, col: str, windows: List[int]):
    """åˆ›å»ºæ»šåŠ¨çª—å£ç‰¹å¾"""

# 2. é¢„æµ‹æ¨¡å‹
def train_arima_model(series, order: Tuple[int, int, int]):
    """è®­ç»ƒARIMAæ¨¡å‹"""
    
def train_lstm_forecaster(X_train, y_train, seq_len: int = 12):
    """è®­ç»ƒLSTMé¢„æµ‹æ¨¡å‹"""

# 3. ä¸šåŠ¡åº”ç”¨
def forecast_tax_revenue(historical_data, periods: int = 12):
    """é¢„æµ‹ç¨æ”¶æ”¶å…¥"""
```

**ä¸šåŠ¡åœºæ™¯**ï¼š
- ç¨æ”¶æ”¶å…¥é¢„æµ‹
- å‘ç¥¨å¼€å…·é‡é¢„æµ‹
- ä¼ä¸šçº³ç¨è¶‹åŠ¿åˆ†æ

---

## ğŸ“‹ å…·ä½“å®æ–½è®¡åˆ’

### é˜¶æ®µ1ï¼šæ ¸å¿ƒAIæŠ€èƒ½ï¼ˆ2-3å‘¨ï¼‰

**Week 1: æœºå™¨å­¦ä¹ åŸºç¡€**
```bash
# åˆ›å»º Set ML1
interview_exercises/set_ML1_blank.py
interview_exercises/set_ML1_answers.py
interview_exercises/set_ML1_answers_annotated.py

# å†…å®¹ï¼š
# - ç‰¹å¾å·¥ç¨‹ï¼ˆ5é¢˜ï¼‰
# - æ¨¡å‹è®­ç»ƒï¼ˆ5é¢˜ï¼‰
# - æ¨¡å‹è¯„ä¼°ï¼ˆ5é¢˜ï¼‰
# - å®æˆ˜ï¼šç¨åŠ¡é£é™©åˆ†ç±»
```

**Week 2: NLPåŸºç¡€**
```bash
# åˆ›å»º Set NLP1
# å†…å®¹ï¼š
# - ä¸­æ–‡åˆ†è¯ä¸é¢„å¤„ç†ï¼ˆ5é¢˜ï¼‰
# - æ–‡æœ¬ç‰¹å¾æå–ï¼ˆ5é¢˜ï¼‰
# - æ–‡æœ¬åˆ†ç±»ï¼ˆ5é¢˜ï¼‰
# - å®æˆ˜ï¼šå‘ç¥¨æè¿°åˆ†ç±»
```

**Week 3: OCRå®æˆ˜**
```bash
# åˆ›å»º Set OCR1
# å†…å®¹ï¼š
# - å›¾åƒé¢„å¤„ç†ï¼ˆ5é¢˜ï¼‰
# - OCRè¯†åˆ«ä¸æå–ï¼ˆ5é¢˜ï¼‰
# - åå¤„ç†ä¸æ ¡éªŒï¼ˆ5é¢˜ï¼‰
# - å®æˆ˜ï¼šå‘ç¥¨æ‰¹é‡è¯†åˆ«
```

### é˜¶æ®µ2ï¼šè¿›é˜¶æŠ€èƒ½ï¼ˆ2-3å‘¨ï¼‰

- Set DL1: æ·±åº¦å­¦ä¹ åŸºç¡€
- Set DEPLOY1: æ¨¡å‹éƒ¨ç½²
- Set DATA_ENG1: æ•°æ®å·¥ç¨‹

### é˜¶æ®µ3ï¼šé«˜çº§æŠ€èƒ½ï¼ˆå¯é€‰ï¼‰

- Set LLM1: å¤§æ¨¡å‹åº”ç”¨
- Set TS1: æ—¶é—´åºåˆ—é¢„æµ‹

---

## ğŸ¯ AIæŠ€èƒ½é€ŸæŸ¥å¡

### æœºå™¨å­¦ä¹ å¸¸ç”¨API

```python
# sklearn åŸºç¡€
from sklearn.model_selection import train_test_split, cross_val_score
from sklearn.preprocessing import StandardScaler, LabelEncoder, OneHotEncoder
from sklearn.linear_model import LogisticRegression
from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import accuracy_score, precision_recall_fscore_support, roc_auc_score

# ç‰¹å¾å·¥ç¨‹
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X_train)

# æ¨¡å‹è®­ç»ƒ
model = LogisticRegression()
model.fit(X_train, y_train)

# è¯„ä¼°
y_pred = model.predict(X_test)
accuracy = accuracy_score(y_test, y_pred)
precision, recall, f1, _ = precision_recall_fscore_support(y_test, y_pred, average='binary')
```

### NLPå¸¸ç”¨API

```python
# jieba åˆ†è¯
import jieba
tokens = jieba.lcut("å¢å€¼ç¨ä¸“ç”¨å‘ç¥¨")

# TF-IDF
from sklearn.feature_extraction.text import TfidfVectorizer
vectorizer = TfidfVectorizer()
X = vectorizer.fit_transform(texts)

# æ–‡æœ¬ç›¸ä¼¼åº¦
from sklearn.metrics.pairwise import cosine_similarity
sim = cosine_similarity(vec1, vec2)
```

### OCRå¸¸ç”¨API

```python
# PaddleOCR
from paddleocr import PaddleOCR
ocr = PaddleOCR(use_angle_cls=True, lang='ch')
result = ocr.ocr(img_path)

# Tesseract
import pytesseract
text = pytesseract.image_to_string(img, lang='chi_sim')
```

---

## âœ… é¢„æœŸæˆæœ

### å®ŒæˆP0å

**AIæŠ€èƒ½è¦†ç›–åº¦**ï¼š
- æœºå™¨å­¦ä¹ åŸºç¡€ï¼š0/10 â†’ **8/10** âœ…
- NLPåŸºç¡€ï¼š2/10 â†’ **7/10** âœ…
- OCRå®æˆ˜ï¼š1/10 â†’ **7/10** âœ…
- ç»¼åˆAIèƒ½åŠ›ï¼š**4/10 â†’ 7.5/10**

**å²—ä½åŒ¹é…åº¦**ï¼š
- ç¨åŠ¡æ‰€AIå·¥ç¨‹å¸ˆï¼š**60% â†’ 85%**
- é€šç”¨AIå·¥ç¨‹å¸ˆï¼š**40% â†’ 70%**

### å®ŒæˆP0+P1å

**AIæŠ€èƒ½è¦†ç›–åº¦**ï¼š
- æ·±åº¦å­¦ä¹ ï¼š0/10 â†’ **6/10** âœ…
- æ¨¡å‹éƒ¨ç½²ï¼š3/10 â†’ **8/10** âœ…
- æ•°æ®å·¥ç¨‹ï¼š2/10 â†’ **7/10** âœ…
- ç»¼åˆAIèƒ½åŠ›ï¼š**7.5/10 â†’ 9/10**

**å²—ä½åŒ¹é…åº¦**ï¼š
- ç¨åŠ¡æ‰€AIå·¥ç¨‹å¸ˆï¼š**85% â†’ 95%**
- é€šç”¨AIå·¥ç¨‹å¸ˆï¼š**70% â†’ 85%**

---

## ğŸš€ ç«‹å³è¡ŒåŠ¨

**æ˜¯å¦éœ€è¦æˆ‘ç«‹å³åˆ›å»ºä»¥ä¸‹å†…å®¹ï¼Ÿ**

1. âœ… **Set ML1**ï¼ˆæœºå™¨å­¦ä¹ åŸºç¡€å¥—é¢˜ï¼‰
2. âœ… **Set NLP1**ï¼ˆNLPåŸºç¡€å¥—é¢˜ï¼‰
3. âœ… **Set OCR1**ï¼ˆOCRå®æˆ˜å¥—é¢˜ï¼‰
4. âœ… **AI_CHEATSHEET.md**ï¼ˆAIæŠ€èƒ½é€ŸæŸ¥å¡ï¼‰
5. âœ… æ›´æ–° `interview_simulator.py` æ·»åŠ  "ai" ä¸»é¢˜
6. âœ… æ›´æ–° `INTERVIEW_SPRINT_GUIDE.md` æ·»åŠ AIæŠ€èƒ½å†²åˆº

**å»ºè®®ä¼˜å…ˆçº§**ï¼š
1. **ç«‹å³åˆ›å»º** Set ML1ï¼ˆæœ€æ ¸å¿ƒï¼‰
2. **æœ¬å‘¨å†…åˆ›å»º** Set NLP1 + Set OCR1
3. **ä¸‹å‘¨åˆ›å»º** Set DL1 + Set DEPLOY1

è¯·å‘Šè¯‰æˆ‘æ˜¯å¦å¼€å§‹åˆ›å»ºè¿™äº›AIä¸“é¡¹å¥—é¢˜ï¼Ÿ

